# Logistic regression and stratified analysis {#exercise3}

```{r "setup3", include = FALSE}
require("knitr")
opts_knit$set(root.dir = "data/")
```

In this exercise we will explore how `R` handles generalised linear models using the example of logistic regression. We will continue using the `salex` dataset. Start `R` and retrieve the `salex` dataset:

```{r, eval = TRUE}
salex <- read.table("salex.dat", header = TRUE, na.strings = "9")
```

When we analysed this data using two-by-two tables and examining the risk ratio and 95% confidence interval associated with each exposure we found many significant positive associations:

+---------------+----------+--------------------+
| **Variable**  | **RR**   | **95% CI**         |
+===============+==========+====================+
| EGGS          | 2.61     | 1.55, 4.38         |
+---------------+----------+--------------------+
| MUSHROOM      | 1.41     | 1.03, 1.93         |
+---------------+----------+--------------------+
| PEPPER        | 1.74     | 1.27, 2.38         |
+---------------+----------+--------------------+
| PASTA         | 1.68     | 1.26, 2.26         |
+---------------+----------+--------------------+
| RICE          | 1.72     | 1.25, 2.34         |
+---------------+----------+--------------------+
| LETTUCE       | 2.01     | 1.49, 2.73         |
+---------------+----------+--------------------+
| COLESLAW      | 1.89     | 1.37, 2.64         |
+---------------+----------+--------------------+
| CHOCOLATE     | 1.39     | 1.05, 1.87         |
+---------------+----------+--------------------+

Some of these associations may be due to *confounding* in the data. We can use logistic regression to help us identify independent associations.

Logistic regression requires the dependent variable to be either 0 or 1. In order to perform a logistic regression we must first recode the `ILL` variable so that 0=no and 1=yes:

```{r, eval = TRUE}
table(salex$ILL)
salex$ILL[salex$ILL == 2] <- 0
table(salex$ILL)
```

We could work with our data as it is but if we wanted to calculate odds ratios and confidence intervals we would calculate their reciprocals (i.e. odds ratios for non-exposure rather than for exposure). This is because of the way the data has been coded (1=yes, 2=no).

In order to calculate meaningful odds ratios the exposure variables should also be coded 0=no, 1=yes. The actual codes used are not important as long as the value used for ‘exposed’ is one greater than the value used for ‘not exposed’.

We could issue a series of commands similar to the one we have just used to recode the `ILL` variable. This is both tedious and unnecessary as the structure of the dataset (i.e. all variables are coded identically) allows us to recode all variables with a single command:

```{r, eval = TRUE}
salex <- read.table("salex.dat", header = TRUE, na.strings = "9")
salex[1:5, ]
salex <- 2 - salex
salex[1:5, ]
```

***WARNING*** : The `attach()` function works with a copy of the data.frame rather than the original data.frame. Commands that manipulate variables in a data.frame may not work as expected if the data.frame has been attached using the `attach()` function.

It is better to manipulate data **_before_** attaching a data.frame. The `detach()` function may be used to remove an attachment prior to any data manipulation.

Many `R` users avoid using the `attach()` function altogether.

We can now use the generalised linear model `glm()` function to specify the logistic regression model:

```{r, eval = TRUE}
salex.lreg <- glm(formula = ILL ~ EGGS + MUSHROOM + PEPPER + PASTA +
                  RICE + LETTUCE + COLESLAW + CHOCOLATE,
                  family = binomial(logit), data = salex)
```

The method used by the `glm()` function is defined by the `family` parameter. Here we specify `binomial` errors and a `logit` (logistic) linking function.

We have saved the output of the `glm()` function in the `salex.lreg` object. We can examine some basic information about the specified model using the `summary()` function:

```{r, eval = TRUE}
summary(salex.lreg)
```

We will use *backwards elimination* to remove non-significant variables from the logistic regression model. Remember that previous commands can be recalled and edited using the up and down arrow keys – they do not need to be typed out in full each time.

`CHOCOLATE` is the least significant variable in the model so we will remove this variable from the model. Storing the output of the `glm()` function is useful as it allows us to use the `update()` function to add, remove, or modify variables without having to describe the model in full:

```{r, eval = TRUE}
salex.lreg <- update(salex.lreg, . ~ . - CHOCOLATE)
summary(salex.lreg)
```

`RICE` is now the least significant variable in the model so we will remove this variable from the model: 

```{r, eval = TRUE}
salex.lreg <- update(salex.lreg, . ~ . - RICE)
summary(salex.lreg)
```

`COLESLAW` is now the least significant variable in the model so we will remove this variable from the model: 

```{r, eval = TRUE}
salex.lreg <- update(salex.lreg, . ~ . - COLESLAW)
summary(salex.lreg)
```

`PEPPER` is now the least significant variable in the model so we will remove this variable from the model: 

```{r, eval = TRUE}
salex.lreg <- update(salex.lreg, . ~ . - PEPPER)
summary(salex.lreg)
```

`MUSHROOM` is now the least significant variable in the model so we will remove this variable from the model: 

```{r, eval = TRUE}
salex.lreg <- update(salex.lreg, . ~ . - MUSHROOM)
summary(salex.lreg)
```

There are now no non-significant variables in the model.

Unfortunately `R` does not present information on the model coefficients in terms of odds ratios and confidence intervals but we can write a function to calculate them for us.

The first step in doing this is to realise that the `salex.lreg` object contains essential information about the fitted model. To calculate odds ratios and confidence intervals we need the regression coefficients and their standard errors. Both:

```{r, eval = TRUE}
summary(salex.lreg)$coefficients
```

and:

```{r, eval = TRUE}
coef(summary(salex.lreg))
```

extract the data that we require. The preferred method is to use the `coef()` function. This is because some fitted models may return coefficients in a more complicated manner than (e.g.) those created by the `glm()` function. The `coef()` function provides a standard way of extracting this data from all classes of fitted objects.

We can store the `coefficients` data in a separate object to make it easier to work with: 

```{r, eval = TRUE}
salex.lreg.coeffs <- coef(summary(salex.lreg))
salex.lreg.coeffs
```

We can extract information from this object by addressing each piece of information by its row and column position in the object. For example:

```{r, eval = TRUE}
salex.lreg.coeffs[2,1]
```

Is the regression coefficient for `EGGS`, and: 

```{r, eval = TRUE}
salex.lreg.coeffs[3,2]
```

is the standard error of the regression coefficient for `PASTA`. Similarly: 

```{r, eval = TRUE}
salex.lreg.coeffs[ ,1]
```

Returns the regression coefficients for all of the variables in the model, and:

```{r, eval = TRUE}
salex.lreg.coeffs[ ,2]
```

Returns the standard errors of the regression coefficients.

The table below shows the indices that address each cell in the table of regression coefficients:

```{r, eval = TRUE}
matrix(salex.lreg.coeffs, nrow = 4, ncol = 4)
```

We can use this information to calculate odds ratio sand 95% confidence intervals:

```{r, eval = TRUE}
or <- exp(salex.lreg.coeffs[ ,1])
lci <- exp(salex.lreg.coeffs[ ,1] - 1.96 * salex.lreg.coeffs[ ,2])
uci <- exp(salex.lreg.coeffs[ ,1] + 1.96 * salex.lreg.coeffs[ ,2])
```

and make a single object that contains all of the required information:

```{r, eval = TRUE}
lreg.or <- cbind(or, lci, uci)
lreg.or
```

We seldom need to report estimates and confidence intervals to more than two decimal places. We can use the `round()` function to remove the excess digits:

```{r, eval = TRUE}
round(lreg.or, digits = 2)
```

We have now gone through all the necessary calculations step-by-step but it would be nice to have a function that did it all for us that we could use whenever we needed to.

First we will create a template for the function:

```{r, eval = FALSE}
lreg.or <- function(model, digits = 2) {}
```

and then use the `fix()` function to edit the `lreg.or()` function:

```{r, eval = FALSE}
fix(lreg.or)
```

We can now edit this function to add a calculation of odds ratios and 95% confidence intervals:

```{r, eval = FALSE}
function(model, digits = 2) {
  lreg.coeffs <- coef(summary(model))
  OR <- exp(lreg.coeffs[ ,1])
  LCI <- exp(lreg.coeffs[ ,1] - 1.96 * lreg.coeffs[ ,2])
  UCI <- exp(lreg.coeffs[ ,1] + 1.96 * lreg.coeffs[ ,2])
  lreg.or <- round(cbind(OR, LCI, UCI), digits = digits)
  lreg.or
}
```

```{r, eval = TRUE}
lreg.or <- function(model, digits = 2) {
  lreg.coeffs <- coef(summary(model))
  OR <- exp(lreg.coeffs[ ,1])
  LCI <- exp(lreg.coeffs[ ,1] - 1.96 * lreg.coeffs[ ,2])
  UCI <- exp(lreg.coeffs[ ,1] + 1.96 * lreg.coeffs[ ,2])
  lreg.or <- round(cbind(OR, LCI, UCI), digits = digits)
  lreg.or
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor.

We can test our function:

```{r, eval = FALSE}
lreg.or(salex.lreg)
```

Which produces the following output:

```{r, eval = TRUE}
lreg.or(salex.lreg)
```

The `digits` parameter of the `lreg.or()` function, which has `digits = 2` as its default value, allows us to specify the precision with which the estimates and their confidence intervals are reported:

```{r, eval = TRUE}
lreg.or(salex.lreg, digits = 4)
```

Before we continue, it is probably a good idea to save this function for later use:

```{r, eval = FALSE}
save(lreg.or, file = "lregor.r")
```

Which can be reloaded whenever it is needed:

```{r, eval = FALSE}
load("lregor.r")
```

An alternative to using logistic regression with data that contains associations that may be due to confounding is to use stratified analysis (i.e. *Mantel-Haenszel* techniques). With several potential confounders, a stratified analysis results in the analysis of many tables which can be difficult to interpret. For example, four potential confounders, each with two levels would produce sixteen tables. In such situations, logistic regression might be a better approach. In order to illustrate Mantel-Haenszel techniques in `R` we will work with a simpler dataset.

On Saturday, 21st April 1990, a luncheon was held in the home of Jean Bateman. There was a total of forty-five guests which included thirty-five members of the Department of Epidemiology and Population Sciences at the London School of Hygiene and Tropical Medicine. On Sunday morning, 22nd April 1990, Jean awoke with symptoms of gastrointestinal illness; her husband awoke with similar symptoms. The possibility of an outbreak related to the luncheon was strengthened when several of the guests telephoned Jean on Sunday and reported illness. On Monday, 23rd April 1990, there was an unusually large number of department members absent from work and reporting illness. Data from this outbreak is stored in the file `bateman.dat`.

The variables in the file `bateman.dat` are:

+---------------+---------------------------------+
| **ILL**       | Ill?                            |
+---------------+---------------------------------+
| **CHEESE**    | Cheddar cheese                  |
+---------------+---------------------------------+
| **CRABDIP**   | Crab dip                        |
+---------------+---------------------------------+
| **CRISPS**    | Crisps                          |
+---------------+---------------------------------+
| **BREAD**     | French bread                    |
+---------------+---------------------------------+
| **CHICKEN**   | Chicken (roasted, served warm)  |
+---------------+---------------------------------+
| **RICE**      | Rice (boiled, served warm)      |
+---------------+---------------------------------+
| **CAESAR**    | Caesar salad                    |
+---------------+---------------------------------+
| **TOMATO**    | Tomato salad                    |
+---------------+---------------------------------+
| **ICECREAM**  | Vanilla ice-cream               |
+---------------+---------------------------------+
| **CAKE**      | Chocolate cake                  |
+---------------+---------------------------------+
| **JUICE**     | Orange juice                    |
+---------------+---------------------------------+
| **WINE**      | White wine                      |
+---------------+---------------------------------+
| **COFFEE**    | Coffee                          |
+---------------+---------------------------------+

Data is available for all forty-five guests at the luncheon. All of the variables are coded 1=yes, 2=no. Retrieve and attach the `bateman` dataset in `R`:

```{r, eval = TRUE}
bateman <- read.table("bateman.dat", header = TRUE)
bateman
attach(bateman)
```

We will use our `tab2by2()` function to analyse this data. Retrieve this function: 

```{r, eval = FALSE}
load("tab2by2.r")
```

Use the `tab2by2()` function to analyse the data:
   
```{r, eval = TRUE}
tab2by2(CHEESE, ILL)
tab2by2(CRABDIP, ILL)
tab2by2(CRISPS, ILL)
tab2by2(BREAD, ILL)
tab2by2(CHICKEN, ILL)
tab2by2(RICE, ILL)
tab2by2(CAESAR, ILL)
tab2by2(TOMATO, ILL)
tab2by2(ICECREAM, ILL)
tab2by2(CAKE, ILL)
tab2by2(JUICE, ILL)
tab2by2(WINE, ILL)
tab2by2(COFFEE, ILL)
```

Two variables (`CAESAR` and `TOMATO`) are associated with `ILL`. 

These two variables are also associated with each other:

```{r, eval = TRUE}
tab2by2(CAESAR, TOMATO)
chisq.test(table(CAESAR, TOMATO))
fisher.test(table(CAESAR, TOMATO))
```

This suggests the potential for one of these associations to be due to confounding. We can perform a simple stratified analysis using the `table()` function:

```{r, eval = TRUE}
table(CAESAR, ILL, TOMATO)
table(TOMATO, ILL, CAESAR)
```

It would be useful to calculate odds ratios for each stratum. We can define a simple function to calculate an odds ratio from a two-by-two table:

```{r, eval = TRUE}
or <- function(x) {(x[1,1] / x[1,2]) / (x[2,1] / x[2,2])}
```

We can use `apply()` to apply the `or()` function to the two-by-two table in each stratum:

```{r, eval = TRUE}
tabC <- table(CAESAR, ILL, TOMATO)
apply(tabC, 3, or)
tabT <- table(TOMATO, ILL, CAESAR)
apply(tabT, 3, or)
```

The 3 instructs the `apply()` function to apply the `or()` function to the third dimension of the table objects (i.e. levels of the potential confounder in `tabC` and `tabT`).

The `mantelhaen.test()` function performs the stratified analysis: 

```{r, eval = TRUE}
mantelhaen.test(tabC)
mantelhaen.test(tabT)
```

It is likely that `CAESAR` salad was a vehicle of food-poisoning, and that `TOMATO` salad was not a vehicle of food-poisoning. Many of those at the luncheon ate both `CAESAR` salad and `TOMATO` salad. `CAESAR` confounded the relationship between `TOMATO` and `ILL`. This resulted in a spurious association between `TOMATO` and `ILL`.

It only makes sense to calculate a common odds ratio in the absence of interaction. We can check for interaction ‘by eye’ by examining and comparing the odds ratios for each stratum as we did above.

There does appear to be an interaction between `CAESAR`, `WINE`, and `ILL`: 

```{r, eval = TRUE}
tabW <- table(CAESAR, ILL, WINE)
apply(tabW, 3, or)
```

*Woolf's test* for interaction (also known as *Woolf's test for the homogeneity of odds ratios*) provides a formal test for interaction.

