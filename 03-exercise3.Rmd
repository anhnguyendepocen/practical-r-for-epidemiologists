# Logistic regression and stratified analysis {#exercise3}

```{r "setup3", include = FALSE}
require("knitr")
opts_knit$set(root.dir = "data/")
```

In this exercise we will explore how `R` handles generalised linear models using the example of logistic regression. We will continue using the `salex` dataset. Start `R` and retrieve the `salex` dataset:

```{r, eval = TRUE}
salex <- read.table("salex.dat", header = TRUE, na.strings = "9")
```

When we analysed this data using two-by-two tables and examining the risk ratio and 95% confidence interval associated with each exposure we found many significant positive associations:

+---------------+----------+--------------------+
| **Variable**  | **RR**   | **95% CI**         |
+===============+==========+====================+
| EGGS          | 2.61     | 1.55, 4.38         |
+---------------+----------+--------------------+
| MUSHROOM      | 1.41     | 1.03, 1.93         |
+---------------+----------+--------------------+
| PEPPER        | 1.74     | 1.27, 2.38         |
+---------------+----------+--------------------+
| PASTA         | 1.68     | 1.26, 2.26         |
+---------------+----------+--------------------+
| RICE          | 1.72     | 1.25, 2.34         |
+---------------+----------+--------------------+
| LETTUCE       | 2.01     | 1.49, 2.73         |
+---------------+----------+--------------------+
| COLESLAW      | 1.89     | 1.37, 2.64         |
+---------------+----------+--------------------+
| CHOCOLATE     | 1.39     | 1.05, 1.87         |
+---------------+----------+--------------------+

Some of these associations may be due to *confounding* in the data. We can use logistic regression to help us identify independent associations.

Logistic regression requires the dependent variable to be either 0 or 1. In order to perform a logistic regression we must first recode the `ILL` variable so that 0=no and 1=yes:

```{r, eval = TRUE}
table(salex$ILL)
salex$ILL[salex$ILL == 2] <- 0
table(salex$ILL)
```

We could work with our data as it is but if we wanted to calculate odds ratios and confidence intervals we would calculate their reciprocals (i.e. odds ratios for non-exposure rather than for exposure). This is because of the way the data has been coded (1=yes, 2=no).

In order to calculate meaningful odds ratios the exposure variables should also be coded 0=no, 1=yes. The actual codes used are not important as long as the value used for ‘exposed’ is one greater than the value used for ‘not exposed’.

We could issue a series of commands similar to the one we have just used to recode the `ILL` variable. This is both tedious and unnecessary as the structure of the dataset (i.e. all variables are coded identically) allows us to recode all variables with a single command:

```{r, eval = TRUE}
salex <- read.table("salex.dat", header = TRUE, na.strings = "9")
salex[1:5, ]
salex <- 2 - salex
salex[1:5, ]
```

***WARNING*** : The `attach()` function works with a copy of the data.frame rather than the original data.frame. Commands that manipulate variables in a data.frame may not work as expected if the data.frame has been attached using the `attach()` function.

It is better to manipulate data **_before_** attaching a data.frame. The `detach()` function may be used to remove an attachment prior to any data manipulation.

Many `R` users avoid using the `attach()` function altogether.

We can now use the generalised linear model `glm()` function to specify the logistic regression model:

```{r, eval = TRUE}
salex.lreg <- glm(formula = ILL ~ EGGS + MUSHROOM + PEPPER + PASTA +
                  RICE + LETTUCE + COLESLAW + CHOCOLATE,
                  family = binomial(logit), data = salex)
```

The method used by the `glm()` function is defined by the `family` parameter. Here we specify `binomial` errors and a `logit` (logistic) linking function.

We have saved the output of the `glm()` function in the `salex.lreg` object. We can examine some basic information about the specified model using the `summary()` function:

```{r, eval = TRUE}
summary(salex.lreg)
```

We will use *backwards elimination* to remove non-significant variables from the logistic regression model. Remember that previous commands can be recalled and edited using the up and down arrow keys – they do not need to be typed out in full each time.

`CHOCOLATE` is the least significant variable in the model so we will remove this variable from the model. Storing the output of the `glm()` function is useful as it allows us to use the `update()` function to add, remove, or modify variables without having to describe the model in full:

```{r, eval = TRUE}
salex.lreg <- update(salex.lreg, . ~ . - CHOCOLATE)
summary(salex.lreg)
```

`RICE` is now the least significant variable in the model so we will remove this variable from the model: 

```{r, eval = TRUE}
salex.lreg <- update(salex.lreg, . ~ . - RICE)
summary(salex.lreg)
```

`COLESLAW` is now the least significant variable in the model so we will remove this variable from the model: 

```{r, eval = TRUE}
salex.lreg <- update(salex.lreg, . ~ . - COLESLAW)
summary(salex.lreg)
```

`PEPPER` is now the least significant variable in the model so we will remove this variable from the model: 

```{r, eval = TRUE}
salex.lreg <- update(salex.lreg, . ~ . - PEPPER)
summary(salex.lreg)
```

`MUSHROOM` is now the least significant variable in the model so we will remove this variable from the model: 

```{r, eval = TRUE}
salex.lreg <- update(salex.lreg, . ~ . - MUSHROOM)
summary(salex.lreg)
```

There are now no non-significant variables in the model.

Unfortunately `R` does not present information on the model coefficients in terms of odds ratios and confidence intervals but we can write a function to calculate them for us.

The first step in doing this is to realise that the `salex.lreg` object contains essential information about the fitted model. To calculate odds ratios and confidence intervals we need the regression coefficients and their standard errors. Both:

```{r, eval = TRUE}
summary(salex.lreg)$coefficients
```

and:

```{r, eval = TRUE}
coef(summary(salex.lreg))
```

extract the data that we require. The preferred method is to use the `coef()` function. This is because some fitted models may return coefficients in a more complicated manner than (e.g.) those created by the `glm()` function. The `coef()` function provides a standard way of extracting this data from all classes of fitted objects.

We can store the `coefficients` data in a separate object to make it easier to work with: 

```{r, eval = TRUE}
salex.lreg.coeffs <- coef(summary(salex.lreg))
salex.lreg.coeffs
```

We can extract information from this object by addressing each piece of information by its row and column position in the object. For example:

```{r, eval = TRUE}
salex.lreg.coeffs[2,1]
```

Is the regression coefficient for `EGGS`, and: 

```{r, eval = TRUE}
salex.lreg.coeffs[3,2]
```

is the standard error of the regression coefficient for `PASTA`. Similarly: 

```{r, eval = TRUE}
salex.lreg.coeffs[ ,1]
```

Returns the regression coefficients for all of the variables in the model, and:

```{r, eval = TRUE}
salex.lreg.coeffs[ ,2]
```

Returns the standard errors of the regression coefficients.

The table below shows the indices that address each cell in the table of regression coefficients:

```{r, eval = TRUE}
matrix(salex.lreg.coeffs, nrow = 4, ncol = 4)
```

We can use this information to calculate odds ratio sand 95% confidence intervals:

```{r, eval = TRUE}
or <- exp(salex.lreg.coeffs[ ,1])
lci <- exp(salex.lreg.coeffs[ ,1] - 1.96 * salex.lreg.coeffs[ ,2])
uci <- exp(salex.lreg.coeffs[ ,1] + 1.96 * salex.lreg.coeffs[ ,2])
```

and make a single object that contains all of the required information:

```{r, eval = TRUE}
lreg.or <- cbind(or, lci, uci)
lreg.or
```

We seldom need to report estimates and confidence intervals to more than two decimal places. We can use the `round()` function to remove the excess digits:

```{r, eval = TRUE}
round(lreg.or, digits = 2)
```

We have now gone through all the necessary calculations step-by-step but it would be nice to have a function that did it all for us that we could use whenever we needed to.

First we will create a template for the function:

```{r, eval = FALSE}
lreg.or <- function(model, digits = 2) {}
```

and then use the `fix()` function to edit the `lreg.or()` function:

```{r, eval = FALSE}
fix(lreg.or)
```

We can now edit this function to add a calculation of odds ratios and 95% confidence intervals:

```{r, eval = FALSE}
function(model, digits = 2) {
  lreg.coeffs <- coef(summary(model))
  OR <- exp(lreg.coeffs[ ,1])
  LCI <- exp(lreg.coeffs[ ,1] - 1.96 * lreg.coeffs[ ,2])
  UCI <- exp(lreg.coeffs[ ,1] + 1.96 * lreg.coeffs[ ,2])
  lreg.or <- round(cbind(OR, LCI, UCI), digits = digits)
  lreg.or
}
```

```{r, eval = TRUE}
lreg.or <- function(model, digits = 2) {
  lreg.coeffs <- coef(summary(model))
  OR <- exp(lreg.coeffs[ ,1])
  LCI <- exp(lreg.coeffs[ ,1] - 1.96 * lreg.coeffs[ ,2])
  UCI <- exp(lreg.coeffs[ ,1] + 1.96 * lreg.coeffs[ ,2])
  lreg.or <- round(cbind(OR, LCI, UCI), digits = digits)
  lreg.or
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor.

We can test our function:

```{r, eval = FALSE}
lreg.or(salex.lreg)
```

Which produces the following output:

```{r, eval = TRUE}
lreg.or(salex.lreg)
```

The `digits` parameter of the `lreg.or()` function, which has `digits = 2` as its default value, allows us to specify the precision with which the estimates and their confidence intervals are reported:

```{r, eval = TRUE}
lreg.or(salex.lreg, digits = 4)
```

Before we continue, it is probably a good idea to save this function for later use:

```{r, eval = FALSE}
save(lreg.or, file = "lregor.r")
```

Which can be reloaded whenever it is needed:

```{r, eval = FALSE}
load("lregor.r")
```

An alternative to using logistic regression with data that contains associations that may be due to confounding is to use stratified analysis (i.e. *Mantel-Haenszel* techniques). With several potential confounders, a stratified analysis results in the analysis of many tables which can be difficult to interpret. For example, four potential confounders, each with two levels would produce sixteen tables. In such situations, logistic regression might be a better approach. In order to illustrate Mantel-Haenszel techniques in `R` we will work with a simpler dataset.

On Saturday, 21st April 1990, a luncheon was held in the home of Jean Bateman. There was a total of forty-five guests which included thirty-five members of the Department of Epidemiology and Population Sciences at the London School of Hygiene and Tropical Medicine. On Sunday morning, 22nd April 1990, Jean awoke with symptoms of gastrointestinal illness; her husband awoke with similar symptoms. The possibility of an outbreak related to the luncheon was strengthened when several of the guests telephoned Jean on Sunday and reported illness. On Monday, 23rd April 1990, there was an unusually large number of department members absent from work and reporting illness. Data from this outbreak is stored in the file `bateman.dat`.

The variables in the file `bateman.dat` are:

+---------------+---------------------------------+
| **ILL**       | Ill?                            |
+---------------+---------------------------------+
| **CHEESE**    | Cheddar cheese                  |
+---------------+---------------------------------+
| **CRABDIP**   | Crab dip                        |
+---------------+---------------------------------+
| **CRISPS**    | Crisps                          |
+---------------+---------------------------------+
| **BREAD**     | French bread                    |
+---------------+---------------------------------+
| **CHICKEN**   | Chicken (roasted, served warm)  |
+---------------+---------------------------------+
| **RICE**      | Rice (boiled, served warm)      |
+---------------+---------------------------------+
| **CAESAR**    | Caesar salad                    |
+---------------+---------------------------------+
| **TOMATO**    | Tomato salad                    |
+---------------+---------------------------------+
| **ICECREAM**  | Vanilla ice-cream               |
+---------------+---------------------------------+
| **CAKE**      | Chocolate cake                  |
+---------------+---------------------------------+
| **JUICE**     | Orange juice                    |
+---------------+---------------------------------+
| **WINE**      | White wine                      |
+---------------+---------------------------------+
| **COFFEE**    | Coffee                          |
+---------------+---------------------------------+

Data is available for all forty-five guests at the luncheon. All of the variables are coded 1=yes, 2=no. Retrieve and attach the `bateman` dataset in `R`:

```{r, eval = TRUE}
bateman <- read.table("bateman.dat", header = TRUE)
bateman
attach(bateman)
```

We will use our `tab2by2()` function to analyse this data. Retrieve this function: 

```{r, eval = FALSE}
load("tab2by2.r")
```

Use the `tab2by2()` function to analyse the data:
   
```{r, eval = TRUE}
tab2by2(CHEESE, ILL)
tab2by2(CRABDIP, ILL)
tab2by2(CRISPS, ILL)
tab2by2(BREAD, ILL)
tab2by2(CHICKEN, ILL)
tab2by2(RICE, ILL)
tab2by2(CAESAR, ILL)
tab2by2(TOMATO, ILL)
tab2by2(ICECREAM, ILL)
tab2by2(CAKE, ILL)
tab2by2(JUICE, ILL)
tab2by2(WINE, ILL)
tab2by2(COFFEE, ILL)
```

Two variables (`CAESAR` and `TOMATO`) are associated with `ILL`. 

These two variables are also associated with each other:

```{r, eval = TRUE}
tab2by2(CAESAR, TOMATO)
chisq.test(table(CAESAR, TOMATO))
fisher.test(table(CAESAR, TOMATO))
```

This suggests the potential for one of these associations to be due to confounding. We can perform a simple stratified analysis using the `table()` function:

```{r, eval = TRUE}
table(CAESAR, ILL, TOMATO)
table(TOMATO, ILL, CAESAR)
```

It would be useful to calculate odds ratios for each stratum. We can define a simple function to calculate an odds ratio from a two-by-two table:

```{r, eval = TRUE}
or <- function(x) {(x[1,1] / x[1,2]) / (x[2,1] / x[2,2])}
```

We can use `apply()` to apply the `or()` function to the two-by-two table in each stratum:

```{r, eval = TRUE}
tabC <- table(CAESAR, ILL, TOMATO)
apply(tabC, 3, or)
tabT <- table(TOMATO, ILL, CAESAR)
apply(tabT, 3, or)
```

The 3 instructs the `apply()` function to apply the `or()` function to the third dimension of the table objects (i.e. levels of the potential confounder in `tabC` and `tabT`).

The `mantelhaen.test()` function performs the stratified analysis: 

```{r, eval = TRUE}
mantelhaen.test(tabC)
mantelhaen.test(tabT)
```

It is likely that `CAESAR` salad was a vehicle of food-poisoning, and that `TOMATO` salad was not a vehicle of food-poisoning. Many of those at the luncheon ate both `CAESAR` salad and `TOMATO` salad. `CAESAR` confounded the relationship between `TOMATO` and `ILL`. This resulted in a spurious association between `TOMATO` and `ILL`.

It only makes sense to calculate a common odds ratio in the absence of interaction. We can check for interaction ‘by eye’ by examining and comparing the odds ratios for each stratum as we did above.

There does appear to be an interaction between `CAESAR`, `WINE`, and `ILL`: 

```{r, eval = TRUE}
tabW <- table(CAESAR, ILL, WINE)
apply(tabW, 3, or)
```

*Woolf's test* for interaction (also known as *Woolf's test for the homogeneity of odds ratios*) provides a formal test for interaction.

`R` does not provide a function to perform *Woolf's test* for the homogeneity of odds ratios but it is possible to write a function to perform this test.

First we will create a template for the function:

```{r, eval = FALSE}
woolf.test <- function(x) {}
```

And then use the `fix()` function to edit the `woolf.test()` function: 

```{r, eval = FALSE}
fix(woolf.test)
```

We can now edit this function to make it do something useful:

```{r, eval = FALSE}
function(x) {
  x <- x + 0.5
  k <- dim(x)[3]
  or <- apply(x, 3, function(x)
              {(x[1, 1] / x[1, 2]) / (x[2, 1] / x[2, 2])})
  w <- apply(x, 3, function(x) {1 / sum(1 / x)})
  chi.sq <- sum(w * (log(or) - weighted.mean(log(or), w))^2)
  p <- pchisq(chi.sq, df = k - 1, lower.tail = FALSE)
  cat("\nWoolf's X2 :", chi.sq,
      "\np-value    :", p, "\n")
}

```{r, echo = FALSE, eval = TRUE}
woolf.test <- function(x) {
  x <- x + 0.5
  k <- dim(x)[3]
  or <- apply(x, 3, function(x)
              {(x[1, 1] / x[1, 2]) / (x[2, 1] / x[2, 2])})
  w <- apply(x, 3, function(x) {1 / sum(1 / x)})
  chi.sq <- sum(w * (log(or) - weighted.mean(log(or), w))^2)
  p <- pchisq(chi.sq, df = k - 1, lower.tail = FALSE)
  cat("\nWoolf's X2 :", chi.sq,
      "\np-value    :", p, "\n")
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor.
We can use the `woolf.test()` function to test for a three-way interaction between `CAESAR`, `WINE`, and `ILL`:

```{r, eval = FALSE}
woolf.test(tabW)
```

Which returns:

```{r, eval = TRUE}
woolf.test(tabW)
```

Which is weak evidence of an interaction.

We should test for interaction between `CAESAR`, `TOMATO`, and `ILL` before accepting the results reported by
the `mantelhaen.test()` function: 

```{r, eval = TRUE}
woolf.test(tabC)
```

We can repeat this analysis using logistic regression.

We need to change the coding of the variables to 0 and 1 before specifying the model:

```{r, eval = FALSE}
detach(bateman)
bateman <- 2 - bateman
bateman
bateman.lreg <- glm(formula = ILL ~ CAESAR + TOMATO,
                    family = binomial(logit), data = bateman)
summary(bateman.lreg)
bateman.lreg <- update(bateman.lreg, . ~ . - TOMATO)
summary(bateman.lreg)
```

```{r, echo = FALSE, eval = TRUE}
detach(bateman)
bateman <- 2 - bateman
bateman
bateman.lreg <- glm(formula = ILL ~ CAESAR + TOMATO,
                    family = binomial(logit), data = bateman)
summary(bateman.lreg)
bateman.lreg <- update(bateman.lreg, . ~ . - TOMATO)
summary(bateman.lreg)
```

Interactions are specified using the multiply (__*__) symbol in the model formula:

```{r, eval = TRUE}
bateman.lreg <- glm(formula = ILL ~ CAESAR + WINE + CAESAR * WINE,
                    family = binomial(logit), data = bateman)
summary(bateman.lreg)
```

Before we continue, it is probably a good idea to save the `woolf.test()` function for later use: 

```{r, eval = FALSE}
save(woolf.test, file = "woolf.r")
```

## Matched data

*Matching* is another way to control for the effects of potential confounding variables. Matching is usually performed during data-collection as part of the design of a study.

In a matched case-control studies, each case is matched with one or more controls which are chosen to have the same values over a set of potential confounding variables.
In order to illustrate how matched data may be analysed using tabulation and stratification in `R` we will start with the simple case of one-to-one matching (i.e. each case has a single matched control):

```{r, eval = TRUE}
octe <- read.table("octe.dat", header = TRUE)
octe[1:10, ]
```

This data is from a matched case-control study investigating the association between oral contraceptive use and thromboembolism. The cases are 175 women aged between 15 and 44 years admitted to hospital for thromboembolism and discharged alive. The controls are female patients admitted for conditions believed to be unrelated to oral contraceptive use. Cases and controls were matched on age, ethnic group, marital status, parity, income, place of residence, and date of hospitalisation. The variables in the dataset are:

+---------------+--------------------------------------------------------------+
| **ID**        | Identifier for the matched sets of cases and controls        |
+---------------+--------------------------------------------------------------+
| **CASE**      | Case (1) or control (2)                                      |
+---------------+--------------------------------------------------------------+
| **OC**        | Used oral contraceptives in the previous month (1=yes, 2=no) |
+---------------+--------------------------------------------------------------+

The dataset consists of 350 records:

```{r, eval = TRUE}
nrow(octe)
```

There are 175 matched sets of cases and controls:

```{r, eval = TRUE}
length(unique(octe$ID))
```

In each matched set of cases and controls there is one case and one control:

```{r, eval = TRUE}
table(octe$ID, octe$CASE)
```

This data may be analysed using *McNemar's chi-squared test* which use the number of discordant (i.e. relative to exposure) pairs of matched cases and controls.

To find the number of discordant pairs we need to split the dataset into cases and controls:

```{r, eval = TRUE}
octe.cases <- subset(octe, CASE == 1)
octe.controls <- subset(octe, CASE == 2)
```

Sorting these two datasets (i.e. `octe.cases` and `octe.controls`) by the `ID` variable simplifies the analysis:

```{r, eval = TRUE}
octe.cases <- octe.cases[order(octe.cases$ID), ]
octe.controls <- octe.controls[order(octe.controls$ID), ]
```

Since the two datasets (i.e. `octe.cases` and `octe.controls`) are now sorted by the `ID` variable we can use the `table()` function to retrieve the number if concordant and discordant pairs and store them in a table object:

```{r, eval = TRUE}
tab <- table(octe.cases$OC, octe.controls$OC)
tab
```

This table object (i.e. `tab`) can then be passed to the `mcnemar.test()` function: 

```{r, eval = TRUE}
mcnemar.test(tab)
```

The `mcnemar.test()` function does not provide an estimate of the odds ratio. This is the ratio of the discordant pairs:

```{r, eval = FALSE}
r <- tab[1,2]
s <- tab[2,1]
rdp <- r / s
rdp
```

```{r, echo = FALSE, eval = TRUE}
r <- tab[1,2]
s <- tab[2,1]
rdp <- r / s
rdp
```

A confidence interval can also be calculated:

```{r, eval = FALSE}
ci.p <- binom.test(r, r + s)$conf.int
ci.rdp <- ci.p / (1 - ci.p)
ci.rdp
```

```{r, echo = FALSE, eval = TRUE}
ci.p <- binom.test(r, r + s)$conf.int
ci.rdp <- ci.p / (1 - ci.p)
ci.rdp
```

This provides a 95% confidence interval. Other (e.g. 99%) confidence intervals can be produced by specifying appropriate values for the `conf.level` parameter of the `binom.test()` function:

```{r, eval = FALSE}
ci.p <- binom.test(r, r + s, conf.level = 0.99)$conf.int
ci.rdp <- ci.p / (1 - ci.p)
ci.rdp
```

```{r, echo = FALSE, eval = TRUE}
ci.p <- binom.test(r, r + s, conf.level = 0.99)$conf.int
ci.rdp <- ci.p / (1 - ci.p)
ci.rdp
```

An alternative way of analysing this data is to use the `mantelhaen.test()` function: 

```{r, eval = FALSE}
tab <- table(octe$OC, octe$CASE, octe$ID)
mantelhaen.test(tab)
```

```{r, echo = FALSE, eval = TRUE}
tab <- table(octe$OC, octe$CASE, octe$ID)
mantelhaen.test(tab)
```

The Mantel-Haenszel approach is preferred because it can be used with data from matched case-control studies that match more than one control to each case. Multiple matching is useful when the condition being studied is rare or at the early stages of an outbreak (i.e. when cases are hard to find and controls are easy to find).

We will now work with some data where each case has one or more controls:

```{r, eval = TRUE}
tsstamp <- read.table("tsstamp.dat", header = TRUE)
tsstamp
```

This data is from a matched case-control study investigating the association between the use of different brands of tampon and toxic shock syndrome undertaken during an outbreak. Only a subset of the original dataset is used here. The variables in the dataset are:

+---------------+------------------------------------------------------------+
| **ID**        | Identifier for the matched sets of cases and controls      |
+---------------+------------------------------------------------------------+
| **CASE**      | Case (1) or control (2)                                    |
+---------------+------------------------------------------------------------+
| **RBTAMP**    | Used Rely brand tampons (1=yes, 2=no)                      |
+---------------+------------------------------------------------------------+

The dataset consists of forty-three (43) records:

```{r, eval = TRUE}
nrow(tsstamp)
```

There are fourteen (14) matched sets of cases and controls:

```{r, eval = TRUE}
length(unique(tsstamp$ID))
```

Each matched set of cases and controls consists of one case and one or more controls:

```{r, eval = TRUE}
table(tsstamp$ID, tsstamp$CASE)
```

The *McNemar's chi-squared test* is not useful for this data as it is limited to the special case of one-to-one matching.

Analysing this data using a simple tabulation such as:

```{r, eval = TRUE}
fisher.test(table(tsstamp$RBTAMP, tsstamp$CASE))
```

ignores the matched nature of the data and is, therefore, also not useful for this data.

The matched nature of the data may be accounted by stratifying on the variable that identifies the matched sets
of cases and controls (i.e. the `ID` variable) using the `mantelhaen.test()` function: 

```{r, eval = TRUE}
mantelhaen.test(table(tsstamp$RBTAMP, tsstamp$CASE, tsstamp$ID))
```

Analysis of several risk factors or adjustment for confounding variables not matched for in the design of a matched case-control study cannot be performed using tabulation-based procedures such as the `McNemar's chi- squared test` and Mantel-Haenszel procedures. In these situations a special form of logistic regression, called `conditional logistic regression`, should be used.

We can now quit `R`: 

```{r, eval = FALSE}
q()
```


For this exercise there is no need to save the workspace image so click the **No** or **Don't Save** button (GUI) or enter n when prompted to save the workspace image (terminal).

## Summary

* `R` provides functions for many kinds of complex statistical analysis. We have looked at using the generalised linear model `glm()` function to perform logistic regression. We have looked ar the `mantelhaen.test()` function to perform stratified analyses and the `mantelhaen.test()` and `mcnemar.test()` functions to analyse data from matched case-control studies.

* `R` can be extended by writing new functions. New functions can perform simple or complex data analysis. New functions can be composed of parts of existing function. New functions can be saved and used in subsequent `R` sessions. By building your own functions you can use `R` to build your own statistical analysis system.

