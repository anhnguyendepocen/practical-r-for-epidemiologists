# Computer intensive methods {#exercise9}

## Estimation

Estimation involves the calculation of a measure with some sense of precision based upon sampling variation.

Only a few estimators (e.g. the sample mean from a normal population) have exact formulae that may be used to estimate sampling variation. Typically, estimates of variability are based upon approximations informed by expected or postulated properties of the sampled population. The development of variance formulae for some measures may require in-depth statistical and mathematical knowledge or may even be impossible to derive.

*Bootstrap* methods are computer-intensive methods that can provide estimates and measures of precision (e.g. confidence intervals) without resort to theoretical models, higher mathematics, or assumptions about the sampled population. They rely on repeated sampling, sometimes called *resampling*, of the observed data.

As a simple example of how such methods work, we will start by using bootstrap methods to estimate the mean from a normal population. We will work with a very simple dataset which we will enter directly:

```{r, eval = TRUE}
x <- c(7.3, 10.4, 14.0, 12.2, 8.4)
```

We can summarise this data quite easily:

```{r, eval = TRUE}
mean(x)
```

The `sample()` function can be used to select a bootstrap `replicate`: 

```{r, eval = TRUE}
sample(x, length(x), replace = TRUE)
```

Enter this command several times to see some more bootstrap replicates. Remember that previous commands can be recalled and edited using the up and down arrow keys â€“ they do not need to be typed out in full each time. The `length()` parameter is not required for taking bootstrap replicates and can be omitted.

It is possible to apply a summary measure to a replicate:

```{r, eval = TRUE}
mean(sample(x, replace = TRUE))
```

Enter this command several times. A bootstrap estimate of the mean of `x` can be made by repeating this process many times and taking the `median` of the means for each replicate.

One way of doing this is to create a matrix where each column contains a bootstrap replicate and then use the `apply()` and `mean()` functions to get at the estimate.

First create the matrix of replicates. Here we take ten replicates:

```{r, eval = TRUE}
x1 <- matrix(sample(x, length(x) * 10, replace = TRUE),
             nrow = length(x), ncol = 10)
x1
```

Then calculate and store the means of each replicate. We can do this using the `apply()` function to apply the `mean()` function to the columns of matrix `x1`:

```{r, eval = TRUE}
x2 <- apply(x1, 2, mean)
x2
```

The bootstrap estimate of the mean is:

```{r, eval = TRUE}
median(x2)
```

The bootstrap estimate may differ somewhat from the mean of `x`: 

```{r, eval = TRUE}
mean(x)
```

The situation is improved by increasing the number of replicates. Here we take 5000 replicates:

```{r, eval = TRUE}
x1 <- matrix(sample(x, length(x) * 5000, replace = TRUE),
             nrow = length(x), ncol = 5000)
x2 <- apply(x1, 2, mean)
median(x2)
```

This is a pretty useless example as estimating the mean / standard deviation, or standard error of the mean of a sample from a normal population can be done using standard formulae.

The utility of bootstrap methods is that they can be applied to summary measures that are not as well understood as the arithmetic mean. The bootstrap method also has the advantage of retaining simplicity even with complicated measures.

To illustrate this, we will work through an example of using the bootstrap to estimate the harmonic mean. 

Again, we will work with a simple dataset which we will enter directly:

```{r, eval = TRUE}
d <- c(43.64, 50.67, 33.56, 27.75, 43.35, 29.56, 38.83, 35.95, 20.01)
```

The data represents distance (in kilometres) from a point source of environmental pollution for nine female patients with oral / pharyngeal cancer.

The harmonic mean is considered to be a sensitive measure of spatial clustering. The first step is to construct a function to calculate the harmonic mean:

```{r, eval = TRUE}
h.mean <- function(x) {length(x) / sum(1 / x)}
```

Calling this function with the sample data:

```{r, eval = TRUE}
h.mean(d)
```

Should return an estimated harmonic mean distance of 33.47 kilometres. This is simple. The problem is that calculating the variance of this estimate is complicated using standard methods. This problem is relatively simple to solve using bootstrap methods:

```{r, eval = TRUE}
replicates <- 5000
n <- length(d)
x1 <- matrix(sample(d, n * replicates, replace = TRUE),
             nrow = n, ncol = replicates)
x2 <- apply(x1, 2, h.mean)
median(x2)
```

A 95% confidence interval can be extracted from `x2` using the `quantile()` function: 

```{r, eval = TRUE}
quantile(x2, c(0.025, 0.975))
```

A 99% confidence interval can also be extracted from `x2` using the `quantile()` function: 

```{r, eval = TRUE}
quantile(x2, c(0.005, 0.995))
```

As a final example of the bootstrap method we will use the method to obtain an estimate of an odds ratio from a two-by-two table. We will work with the `salex` dataset which we used in exercise 2 and exercise 3:

```{r, eval = TRUE}
salex <- read.table("salex.dat",  header = TRUE, na.strings = "9")
table(salex$EGGS, salex$ILL)
```

We should set up our estimator function to calculate an odds ratio from a two-by-two table:

```{r, eval = TRUE}
or <- function(x) {(x[1,1] / x[1,2]) / (x[2,1] / x[2,2])}
```

We should test this:

```{r, eval = TRUE}   
or(table(salex$EGGS, salex$ILL))
```

The problem is to take a bootstrap replicate from two vectors in a data.frame. This can be achieved by using `sample()` to create a vector of row indices and then use this sample of indices to select replicates from the data.frame:
   
```{r, eval = TRUE}
boot <- NULL
for(i in 1:1000) {
  sampled.rows <- sample(1:nrow(salex), replace = TRUE)
  x <- salex[sampled.rows, "EGGS"]
  y <- salex[sampled.rows, "ILL"]
  boot[i] <- or(table(x, y))
}
```

The vector `boot` now contains the odds ratios calculated from 1000 replicates. Estimates of the odds ratio and its 95% confidence interval may be obtained using the `median()` and `quantile()` functions

```{r, eval = FALSE}   
median(boot)
quantile(boot, c(0.025, 0.975))
```

```{r, echo = FALSE, eval = TRUE}   
median(boot)
quantile(boot, c(0.025, 0.975))
```

This approach may fail when some tables have cells that contain zero. Infinite values arise due to division by zero when calculating the odds ratio for some replicates. We can avoid this problem by selecting only those values of `boot` that are not (`!=`) infinite (`Inf`):

```{r, eval = FALSE}
boot <- boot[boot != Inf]
median(boot)
quantile(boot, c(0.025, 0.975))
```

```{r, echo = FALSE, eval = TRUE}
boot <- boot[boot != Inf]
median(boot)
quantile(boot, c(0.025, 0.975))
```

Another way to avoid this problem is to use an `adjusted odds ratio` calculated by adding 0.5 to each cell of the two-by-two table:

```{r, eval = FALSE}
boot <- NULL
for(i in 1:1000) {
  sampled.rows <- sample(1:nrow(salex), replace = TRUE)
  x <- salex[sampled.rows, "EGGS"]
  y <- salex[sampled.rows, "ILL"]
  boot[i] <- or(table(x, y) + 0.5)
  }
median(boot)
quantile(boot, c(0.025, 0.975))
```

```{r, echo = FALSE, eval = TRUE}
boot <- NULL
for(i in 1:1000) {
  sampled.rows <- sample(1:nrow(salex), replace = TRUE)
  x <- salex[sampled.rows, "EGGS"]
  y <- salex[sampled.rows, "ILL"]
  boot[i] <- or(table(x, y) + 0.5)
  }
median(boot)
quantile(boot, c(0.025, 0.975))
```

This procedure is preferred when working with sparse tables.


## Hypothesis testing

Computer-intensive methods also offer a general approach to statistical hypothesis testing. To illustrate this we will use *computer based simulation* to investigate spatial clustering around a point.

Before continuing, we will retrieve a dataset:

```{r, eval = TRUE}
waste <- read.table("waste.dat", header = TRUE)
```

The file `waste.dat` contains the location of twenty-three recent cases of childhood cancer in 5 by 5 km square surrounding an industrial waste disposal site. The columns in the dataset are:

+----------+------------------------------+
| **x**    | The x location of cases      |
+----------+------------------------------+
| **y**    | The y location of cases      |
+----------+------------------------------+

The `x` and `y` variables have been transformed to lie between 0 and 1 with the industrial waste disposal site centrally located (i.e. at `x` = 0.5, `y` = 0.5).

Plot the data and the location of the industrial waste disposal site on the same chart:

```{r, eval = TRUE}
plot(waste$x, waste$y, xlim = c(0, 1), ylim = c(0, 1))
points(0.5, 0.5, pch = 3)
```

We can calculate the distance of each point from the industrial waste disposal site using Pythagoras' Theorem:

```{r, eval = TRUE}
distance.obs <- sqrt((waste$x - 0.5) ^ 2 + (waste$y - 0.5) ^ 2)
```

The observed mean distance or each case from the industrial waste disposal site is:

```{r, eval = TRUE}   
mean(distance.obs)
```

To test whether this distance is unlikely to have arisen by chance (i.e. evidence of spatial clustering) we need to simulate the distribution of distances when no spatial pattern exists:

```{r, eval = TRUE}   
r <- 10000
x.sim <- matrix(runif(r * 23), 23, r)
y.sim <- matrix(runif(r * 23), 23, r)
distance.run <- sqrt((x.sim - 0.5)^2 + (y.sim - 0.5)^2)
distance.sim <- apply(distance.run, 2, mean)
hist(distance.sim, breaks = 20)
abline(v = mean(distance.obs), lty = 3)
```

The probability (i.e. the *p-value*) of observing a mean distance smaller than the *observed mean* distance under the null hypothesis can be estimated as the number of estimates of the mean distance under the null hypothesis falling below the observed mean divided by the total number of estimates of the mean distance under the null hypothesis:

```{r, eval = TRUE}
m <- mean(distance.obs)
z <- ifelse(distance.sim < m, 1, 0)
sum(z) / r
```

You might like to repeat this exercise using the harmonic mean distance and the median distance.

We can check if this method is capable of detecting a simple cluster using simulated data:

```{r, eval = TRUE}
x <- rnorm(23, mean = 0.5, sd = 0.2)
y <- rnorm(23, mean = 0.5, sd = 0.2)
plot(x, y, xlim = c(0, 1), ylim = c(0, 1))
points(0.5, 0.5, pch = 3)
```

We need to recalculate the distance of each simulated case from the industrial waste disposal site:

```{r, eval = TRUE}
distance.obs <- sqrt((x - 0.5) ^ 2 + (y - 0.5) ^ 2)
```

The observed mean distance of each case from the industrial waste disposal site is:
   
```{r, eval = TRUE}   
mean(distance.obs)
```

We can use the the simulated null hypothesis data to test for spatial clustering:

```{r, eval = TRUE}
m <- mean(distance.obs)
z <- ifelse(distance.sim < m, 1, 0)
sum(z) / r
```

We should also check if the procedure can detect a plume of cases, such as might be created by a prevailing wind at a waste incineration site, in a similar way:

```{r, eval = TRUE}
x <- rnorm(23, 0.25, 0.1) + 0.5
y <- rnorm(23, 0.25, 0.1) + 0.5
plot(x, y, xlim = c(0, 1), ylim = c(0, 1))
points(0.5, 0.5, pch = 3)
distance.obs <- sqrt((x - 0.5)^2 + (y - 0.5)^2)
m <- mean(distance.obs)
z <- ifelse(distance.sim < m, 1, 0)
sum(z) / r
```

The method is not capable of detecting plumes.

You might like to try adapting the simulation code presented here to provide a method capable of detecting plumes of cases.

## Simulating processes

In the previous example we simulated the expected distribution of data under the *null hypothesis*. Computer based simulations are not limited to simulating data. They can also be used to simulate processes.

In this example we will simulate the behaviour of the *lot quality assurance sampling* (LQAS) survey method when sampling constraints lead to a loss of sampling independence. In this example the sampling process is simulated and applied to real-world data.

LQAS is a small-sample classification technique that is widely used in manufacturing industry to judge the quality of a batch of manufactured items. In this context, LQAS is used to identify batches that are likely to contain an unacceptably large number of defective items. In the public health context, LQAS may be used to identify communities with unacceptably low levels of service (e.g. vaccine) coverage or worrying levels of disease prevalence.

The LQAS method produces data that is easy to analyse. Data analysis is performed as data is collected and consists solely of counting the number of *defects* (e.g. children with a specific disease) in the sample and checking whether a predetermined threshold value has been exceeded. This combination of data collection and data analysis is called a *sampling plan*. LQAS sampling plans are developed by specifying:


- **A TRIAGE SYSTEM**: A classification system that defines *high*, *moderate*, and *low* categories of the prevalence of the phenomenon of interest.

- **ACCEPTABLE PROBABILITIES OF ERROR**: There are two probabilities of error. These are termed provider *probability of error* (PPE) and *consumer probability of error* (CPE):

    - **Provider Probability of Error (PPE)**: The risk that the survey will indicate that prevalence is *high* when it is, in fact, *low*. PPE is analogous to *type I* ($\alpha$) error in statistical hypothesis testing.
    
    
    - **Consumer Probability of Error (CPE)**: The risk that the survey will indicate that prevalence is *low* when it is, in fact, *high*. CPE is analogous to *type II* ($\beta$) error in statistical hypothesis testing.
    
Once the upper and lower levels of the triage system and acceptable levels of error have been decided, a set of probability tables are constructed that are used to select a maximum sample size (`n`) and the number of defects or cases (`d`) that are allowed in the sample of `n` subjects before deciding that a population is a high prevalence population. The combination of maximum sample size (`n`) and number of defects (`d`) form the stopping rules of the sampling plan. Sampling stops when either the maximum sample size (`n`) is met or the allowable number of defects (`d`) is exceeded:

* If `d` is exceeded then the population is classified as high prevalence.

* If `n` is met without d being exceeded then the population is classified as low prevalence.

The values of `n` and `d` used in a sampling plan depend upon the threshold values used in the triage system and the acceptable levels of error. The values of `n` and `d` used in a sampling plan are calculated using binomial probabilities. For example, the probabilities of finding 14 or fewer cases ($d = 14$) in a sample of 50 individuals ($n = 50$) from populations with prevalences of either 20% or 40% are:

```{r, eval = FALSE}
pbinom(q = 14, size = 50, prob = 0.2)
pbinom(q = 14, size = 50, prob = 0.4)
```

```{r, echo = FALSE, eval = TRUE}
pbinom(q = 14, size = 50, prob = 0.2)
pbinom(q = 14, size = 50, prob = 0.4)
```

The sampling plan with $n = 50$ and $d = 14$ is, therefore, a reasonable candidate for a sampling plan intended to distinguish between populations with prevalences of less than or equal to 20% and populations with prevalences greater than or equal to 40%.

There is no middle ground with LQAS sampling plans. Population are always classified as either high or low prevalence. Populations with prevalences between the upper and lower standards of the triage system are classified as high or low prevalence populations. The probability of a moderate prevalence population being classified as high or low prevalence is proportional to the proximity of the prevalence in that population to the triage standards. Moderate prevalence populations close to the upper standard will tend to be classified as high prevalence populations. Moderate prevalence populations close to the lower standard will tend to be classified as low prevalence populations. This behaviour is summarised by the operating characteristic (OC) curve for the sampling plan. For example:

```{r, eval = TRUE}
plot(seq(0, 0.6, 0.01),
     pbinom(14, 50, seq(0, 0.6, 0.01), lower.tail = FALSE),
     main = "OC Curve for n = 50, d = 14",
     xlab = "Proportion diseased",
     ylab = "Probability",
     type = "l", lty = 2)
```

The data we will use for the simulation is stored in forty-eight separate files. These files contain the returns from whole community screens for active trachoma (TF/TI) in children undertaken as part of trachoma control activities in five African countries. Each file has the file suffix .sim. The name of the file reflects the country in which the data were collected. The data files are:

+--------------------+-----------+--------------------+
| **File name**      | **Files** | **Origin**         |
+:===================+==========:+:===================+
| **egyptXX.sim**    | 10        | Egypt              |
+--------------------+-----------+--------------------+
| **gambiaXX.sim**   | 10        | Gambia             |
+--------------------+-----------+--------------------+
| **ghanaXX.sim**    | 3         | Ghana              |
+--------------------+-----------+--------------------+
| **tanzaniaXX.sim** | 14        | Tanzania           |
+--------------------+-----------+--------------------+
| **togoXX.sim**     | 11        | Togo               |
+--------------------+-----------+--------------------+

All of these data files have the same structure. The variables in the data files are:

+---------------+--------------------------------------------------+
| **hh**        | Household identifier                             |
+---------------+--------------------------------------------------+
| **sex**       | Sex of child (1=male, 2=female)                  |
+---------------+--------------------------------------------------+
| **age**       | Age of child in years                            |
+---------------+--------------------------------------------------+
| **tfti**      | Child has active (TF/TI) trachoma (0=no, 1=yes)  |
+---------------+--------------------------------------------------+

Each row in these files represents a single child. For example:

```{r, eval = TRUE}
x <- read.table("gambia09.sim", header = TRUE)
x[1:10, ]
```

Any rapid survey method that is appropriate for general use in developing countries is restricted to sampling `households` rather than `individuals`. Sampling households in order to sample individuals violates a principal requirement for a sample to be representative of the population from which it is drawn (i.e. that individuals are selected `independently` of each other). This lack of statistical independence amongst sampled individuals may invalidate standard approaches to selecting sampling plans leading to increased probabilities of error. This is likely to be a particular problem if cases tend to be clustered within households. Trachoma is an infectious disease that confers no lasting immunity in the host. Cases are, therefore, very likely to be clustered within households. One solution to this problem would be to sample (i.e. at random) a single child from each of the sampled households. This is not appropriate for active trachoma as the examination procedure often causes distress to younger children. This may influence survey staff to select older children, who tend to have a lower risk of infection, for examination. Sampling is, therefore, constrained to sampling all children in selected households.

The purpose of the simulations presented here is to determine whether the LQAS method is robust to:


1. The loss of sampling independence introduced by sampling households at random and examining all children in selected households for active trachoma.

And:

2. The slight increase in the maximum sample size (`n`) introduced by examining all children in selected households for active trachoma.

Each row in the datasets we will be using represents an individual child. Since we will simulate sampling households rather than individual children we need to be able to convert the datasets from one row per child to one row per household. We will write a function to do this.

Create a new function called `ind2hh()`: 

```{r, eval = FALSE}
ind2hh <- function() {}
```

This creates an empty function called `ind2hh()`. Use the `fix()` function to edit the `ind2hh()` function: 

```{r, eval = FALSE}
fix(ind2hh)
```

Edit the function to read:

```{r, eval = FALSE}
function(data) {
  n.kids <- n.cases <- NULL
  id <- unique(data$hh)
  for(household in id) {
    temp <- subset(data, data$hh == household)
    n.kids <- c(n.kids, nrow(temp))
    n.cases <- c(n.cases, sum(temp$tfti))
  }
  result <- as.data.frame(cbind(id, n.kids, n.cases))
  return(result)
}
```

```{r, echo = FALSE, eval = TRUE}
ind2hh <- function(data) {
  n.kids <- n.cases <- NULL
  id <- unique(data$hh)
  for(household in id) {
    temp <- subset(data, data$hh == household)
    n.kids <- c(n.kids, nrow(temp))
    n.cases <- c(n.cases, sum(temp$tfti))
  }
  result <- as.data.frame(cbind(id, n.kids, n.cases))
  return(result)
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor.

Now we have created the `ind2hh()` function we should test it for correct operation. We will create a simple test data.frame (`test.df`) for this purpose:

```{r, eval = TRUE}
test.df <- as.data.frame(cbind(c(1, 1, 2, 2, 2),  c(1, 1, 1, 0 ,0)))
names(test.df) <- c("hh", "tfti")
test.df
```

The expected operation of the `ind2hh()` function given `test.df` as input is:

+-----+----------+
| hh  | tfti     |
+-----+----------+
| 1   | 1        |
+-----+----------+
| 1   | 1        |
+-----+----------+
| 2   | 1        |
+-----+----------+
| 2   | 0        |
+-----+----------+
| 2   | 0        |
+-----+----------+

becomes

+-----+----------+----------+
| id  | n.kids   | n.case   |
+-----+----------+----------+
| 1   | 2        | 2        |
+-----+----------+----------+
| 2   | 3        | 1        |
+-----+----------+----------+

Confirm this behaviour:

```{r, eval = FALSE}
test.df
ind2hh(test.df)
```

```{r, echo = FALSE, eval = TRUE}
test.df
ind2hh(test.df)
```

We can apply this function to the datasets as required. For example:

```{r, eval = FALSE}
x <- read.table("gambia09.sim", header = TRUE)
x
x.hh <- ind2hh(x)
x.hh
```

```{r, echo = FALSE, eval = TRUE}
x <- read.table("gambia09.sim", header = TRUE)
x
x.hh <- ind2hh(x)
x.hh
```

We will now write a function that will simulate a single LQAS survey. Create a new function called `lqas.run()`:

```{r, eval = FALSE}
lqas.run <- function() {}
```

This creates an empty function called `lqas.run()`.

Use the `fix()` function to edit the `lqas.run()` function:

```{r, eval = FALSE}
fix(lqas.run)
```


Edit the function to read:

```{r, echo = FALSE, eval = TRUE}
lqas.run <- function(x, n, d) {
  kids <- cases <- 0
  y <- x[sample(1:nrow(x),replace = TRUE), ]
  for(i in 1:nrow(y)) {
    kids <- kids + y[i, ]$n.kids
    cases <- cases + y[i, ]$n.cases
    if(cases > d) {
      outcome = 1
       break
    }
    if(kids >= n & cases <= d) {
      outcome = 0
      break
    }
  }
  result <- list(kids = kids, cases = cases, outcome = outcome)
  return(result)
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor.

We should try this function on a low, a moderate, and a high prevalence dataset. To select suitable test datasets we need to know the prevalence in each dataset:

```{r, eval = TRUE}
for(i in dir(pattern = "\\.sim$")) {
  data <- read.table(i, header = TRUE)
  cat(i, ":", mean(data$tfti), "\n")
}
```

The coding scheme used for the `tfti` variable (0=no, 1=yes) allows us to use the `mean()` function to calculate prevalence in these datasets.

The pattern `"\\.sim$"` is a regular expression for files ending in .sim.

If you want to use the `dir()` function to list files stored outside of the current working directory you will need to specify an appropriate value for the `path` parameter. On some systems you may also need to set the value of the `full.names` parameter to `TRUE`. For example:

```{r, eval = FALSE}
for(i in dir(path = "~/prfe", pattern = "\\.sim$", full.names = TRUE)) {
  data <- read.table(i, header = TRUE)
  cat(i, ":", mean(data$tfti), "\n")
}
```

cycles through all files ending in `.sim` (specified by giving the value `"\\.sim$"` to the `pattern` parameter) that are stored the `prfe` directory under the users home directory (specified by giving the value `"~/prfe"` to the `path` parameter) on UNIX systems. You **cannot** usefully specify a URL for the `path` parameter of the `dir()` function.

We will use `tanzania04.sim` as an example of a low prevalence dataset:

```{r, eval = TRUE}
x <- read.table("tanzania04.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.run(x = x.hh, n = 50, d = 14)
```

Repeat the last function call several times. Remember that previous commands can be recalled and edited using the up and down arrow keys â€“ they do not need to be typed out in full each time.

The function should, for most calls, return:

```
$outcome
[1] 0
```

We will use `tanzania08.sim` as an example of a high prevalence dataset:

```{r, eval = TRUE}
x <- read.table("tanzania08.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.run(x = x.hh, n = 50, d = 14)
```

Repeat the last function call several times. The function should, for most calls, return:

```
$outcome
[1] 1
```

We will use `tanzania06.sim` as an example of a moderate prevalence dataset:
   
```{r, eval = TRUE}
x <- read.table("tanzania06.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.run(x = x.hh, n = 50, d = 14)
```

Repeat the last function call several times. The function should return:

```
$outcome
[1] 0
```

And:

```   
$outcome
[1] 1
```

In roughly equal proportion.

The simulation will require repeated sampling from the same dataset. We need to write a function to do this.

Create a new function called `lqas.simul()`: 

```{r, eval = FALSE}
lqas.simul <- function() {}
```

This creates an empty function called `lqas.simul()`.

Use the `fix()` function to edit the `lqas.simul()` function:

```{r, eval = FALSE}
fix(lqas.simul)
```

Edit the function to read:

```{r, eval = FALSE}
function(x, n, d, runs) {
  all <- data.frame()
  for(i in 1:runs) {
    run <- data.frame(lqas.run(x, n ,d))
    all <- rbind(all, run)
  }
  p <- sum(x$n.cases) / sum(x$n.kids)
  asn <- mean(all$kids)
  p.high <- mean(all$outcome)
  result <- list(p = p, asn = asn, p.high = p.high)
  return(result)
}
```

```{r, echo = FALSE, eval = TRUE}
lqas.simul <- function(x, n, d, runs) {
  all <- data.frame()
  for(i in 1:runs) {
    run <- data.frame(lqas.run(x, n ,d))
    all <- rbind(all, run)
  }
  p <- sum(x$n.cases) / sum(x$n.kids)
  asn <- mean(all$kids)
  p.high <- mean(all$outcome)
  result <- list(p = p, asn = asn, p.high = p.high)
  return(result)
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor.

We can test this function with the same three test datasets:

```{r, eval = FALSE}
x <- read.table("tanzania04.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.simul(x = x.hh, n = 50, d = 14, runs = 250)

x <- read.table("tanzania08.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.simul(x = x.hh, n = 50, d = 14, runs = 250)

x <- read.table("tanzania06.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.simul( x = x.hh, n = 50, d = 14, runs = 250)
```

```{r, echo = FALSE, eval = TRUE}
x <- read.table("tanzania04.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.simul(x = x.hh, n = 50, d = 14, runs = 250)

x <- read.table("tanzania08.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.simul(x = x.hh, n = 50, d = 14, runs = 250)

x <- read.table("tanzania06.sim", header = TRUE)
x.hh <- ind2hh(x)
lqas.simul( x = x.hh, n = 50, d = 14, runs = 250)
```

The simulation consists of applying this function to each of the datasets in turn and collating the results. We will create a function to do this.

Create a new function called `main.simul()`: 

```{r, eval = FALSE}
main.simul <- function() {}
```

This creates an empty function called `main.simul()`.

Use the `fix()` function to edit the `main.simul()` function:

```{r, eval = FALSE}
fix(main.simul)
```

Edit the function to read:

```{r, eval = FALSE}
function(n, d, runs) {
  result <- data.frame()
  for(i in dir(pattern = "\\.sim$")) {
    cat(".", sep = "")
    x <- read.table(i, header = TRUE)
    y <- ind2hh(x)
    z <- lqas.simul(y, n, d, runs)
    z$file.name <- i
    result <- rbind(result, as.data.frame(z))
  }
  return(result)
}
```

```{r, echo = FALSE, eval = TRUE}
main.simul <- function(n, d, runs) {
  result <- data.frame()
  for(i in dir(pattern = "\\.sim$")) {
    cat(".", sep = "")
    x <- read.table(i, header = TRUE)
    y <- ind2hh(x)
    z <- lqas.simul(y, n, d, runs)
    z$file.name <- i
    result <- rbind(result, as.data.frame(z))
  }
  return(result)
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor. 

We are now ready to run the simulation:

```{r, eval = TRUE}
z1 <- main.simul(n = 50, d = 14, runs = 250)
```

Progress of the simulation is shown by a lengthening line of dots. Each dot represents one community screening file processed. The `>` prompt will be displayed when the simulation has finished running.

The returned data.frame object (`z1`) contains the results of the simulation:

```{r, eval = TRUE}
z1
```

We can examine the prevalences in the test datasets:

```{r, eval = FALSE}
x11()

hist(z1$p,
     main = "Prevalence in test datasets",
     xlab = "Proportion TF/TI")
```

```{r, echo = FALSE, eval = TRUE}
#x11()

hist(z1$p,
     main = "Prevalence in test datasets",
     xlab = "Proportion TF/TI")
```


If you are using a Macintosh computer then you can use `quartz()` instead of `x11()`. This will give better results.

We examine the sample size required to make classifications at different levels of prevalence as *anaverage sample number* (ASN) curve:

```{r, eval = FALSE}
x11()

plot(z1$p,
     z1$asn,
     main = "ASN Curve",
     xlab = "Proportion TF/TI",
     ylab = "Sample size required")
```

```{r, echo = FALSE, eval = TRUE}
#x11()

plot(z1$p,
     z1$asn,
     main = "ASN Curve",
     xlab = "Proportion TF/TI",
     ylab = "Sample size required")
```

We can examine the performance of the sampling plan by plotting its *operating characteristic* (OC) curve:

```{r, eval = FALSE}
x11()

plot(z1$p,
     z1$p.high,
     main = "OC Curve",
     xlab = "Proportion TF/TI",
     ylab = "Probability")
```

```{r, echo = FALSE, eval = TRUE}
#x11()

plot(z1$p,
     z1$p.high,
     main = "OC Curve",
     xlab = "Proportion TF/TI",
     ylab = "Probability")
```

Before closing the OC curve plot, we can compare the simulation results with the expected *operating characteristic* (OC) curve under ideal sampling conditions:

```{r, eval = FALSE}
lines(seq(0, max(z1$p), 0.01),
      pbinom(14, 50,seq(0, max(z1$p),0.01), lower.tail = FALSE),
      lty = 3)
```

```{r, echo = FALSE, eval = TRUE}
#x11()

plot(z1$p,
     z1$p.high,
     main = "OC Curve",
     xlab = "Proportion TF/TI",
     ylab = "Probability")

lines(seq(0, max(z1$p), 0.01),
      pbinom(14, 50,seq(0, max(z1$p),0.01), lower.tail = FALSE),
      lty = 3)
```

The LQAS method appears to be robust to the loss of sampling variation introduced by the proposed sampling constraints. There is, however, some deviation from the expected *operating characteristic* (OC) curve at lower prevalences:

```{r, eval = FALSE}
x11()
plot(z1$p, z1$p.high,
     xlim = c(0.10, 0.35),
     main = "OC Curve",
     xlab = "Proportion TF/TI",
     ylab = "Probability")

lines(seq(0.10, 0.35, 0.01),
      pbinom(14, 50, seq(0.10, 0.35, 0.01), lower.tail = FALSE),
      lty = 3)
```

```{r, echo = FALSE, eval = TRUE}
#x11()
plot(z1$p, z1$p.high,
     xlim = c(0.10, 0.35),
     main = "OC Curve",
     xlab = "Proportion TF/TI",
     ylab = "Probability")

lines(seq(0.10, 0.35, 0.01),
      pbinom(14, 50, seq(0.10, 0.35, 0.01), lower.tail = FALSE),
      lty = 3)
```

This deviation from the expected *operating characteristic* (OC) curve is likely to be due to a few very large households in which many of the children have active trachoma. You can check this by examining the `p.high` (i.e. the probability of a classification as high prevalence) column in `z1`, and household size and trachoma status in the individual data files that return higher than expected values for `p.high`. The `ind2hh()` function is likely to prove useful in this context.

The observed deviation from the expected *operating characteristic* (OC) curve is small but it is important, in the resource-constrained context of trachoma-endemic countries, to minimise the false positive rate in order to ensure that resources are devoted to communities that need them most.

We might be able to improve the performance of the survey method in this regard by restricting the sample so that only younger children are examined. This will have the effect of reducing the number of children examined in each household. It also has the benefit of making the surveys simpler and quicker since younger children will tend to be closer to home than older children.

The range of ages in the datasets can be found using:

```{r, eval = TRUE}
for(i in dir(pattern = "\\.sim$")) {
  x <- read.table(i, header = TRUE)
  cat(i, ":", range(x$age), "\n")
}
```

We will investigate the effect of restricting the sample to children aged between two and five years. Use the fix() function to edit the `main.simul()` function:

```{r, eval = FALSE}
fix(main.simul)
```

Edit the function to read:

```{r, eval = FALSE}
function(n, d, runs) {
  result <- data.frame()
  for(i in dir(pattern = "\\.sim$")) {
    cat(".", sep = "")
    x <- read.table(i, header = TRUE)
    x <- subset(x, age >= 2 & age <= 5)
    y <- ind2hh(x)
    z <- lqas.simul(y, n, d, runs) z$file.name <- i
    result <- rbind(result, as.data.frame(z))
  }
  return(result)
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor. 

We are now ready to run the simulation again:

```{r, eval = TRUE}
z2 <- main.simul(n = 50, d = 14, runs = 250)
```

The data.frame object `z2` contains the results of the simulation:

```{r, eval = TRUE}
z2
```

We can examine the performance of the sampling plan on the age-restricted datasets by plotting its *operating characteristic* (OC) curve and comparing the simulation results with the expected *operating characteristic* (OC) curve under ideal sampling conditions:

```{r, eval = FALSE}
x11()

plot(z2$p, z2$p.high,
     main = "OC Curve",
     xlab = "Proportion TF/TI",
     ylab = "Probability ")

lines(seq(0, max(z2$p), 0.01),
      pbinom(14, 50, seq(0, max(z2$p), 0.01), lower.tail = FALSE),
      lty = 3)
```

```{r, echo = FALSE, eval = TRUE}
#x11()

plot(z2$p, z2$p.high,
     main = "OC Curve",
     xlab = "Proportion TF/TI",
     ylab = "Probability ")

lines(seq(0, max(z2$p), 0.01),
      pbinom(14, 50, seq(0, max(z2$p), 0.01), lower.tail = FALSE),
      lty = 3)
```

Remember that if you are using a Macintosh computer then you can use `quartz()` instead of `x11()`. This will give better results.

We should also take a closer look at the range of prevalences where the deviation from the expected *operating characteristic* (OC) curve was largest and most problematic in the previous simulation:

```{r, eval = FALSE}
x11()

plot(z2$p, z2$p.high,
     xlim = c(0.10, 0.35),
     main = "OC Curve",
     xlab = "Proportion TF/TI",
     ylab = "Probability")

lines(seq(0.10, 0.35, 0.01),
      pbinom(14, 50, seq(0.10, 0.35, 0.01), lower.tail = FALSE),
      lty = 3)
```

```{r, echo = FALSE, eval = TRUE}
#x11()

plot(z2$p, z2$p.high,
     xlim = c(0.10, 0.35),
     main = "OC Curve",
     xlab = "Proportion TF/TI",
     ylab = "Probability")

lines(seq(0.10, 0.35, 0.01),
      pbinom(14, 50, seq(0.10, 0.35, 0.01), lower.tail = FALSE),
      lty = 3)
```

We can compare the behaviour of the sampling plan in the unrestricted and age-restricted datasets:

```{r, eval = FALSE}
x11()

plot(z1$p, z1$p.high,
     main = "",
     xlab = "",
     ylab = "",
     axes = FALSE,
     xlim = c(0, 0.8))

par(new = TRUE)

plot(z2$p, z2$p.high,
     main = "OC Curve",
     xlab = "Proportion TF/TI",
     ylab = "Probability",
     xlim = c(0, 0.8),
     pch = 3)
   
lines(seq(0, 0.8, 0.01),
      pbinom(14, 50, seq(0, 0.8, 0.01), lower.tail = FALSE),
      lty = 3)
```

Restricting the sample to children aged between two and five years (inclusive) appears to have improved the behaviour of the survey method by lowering the false positive rate to close to expected behaviour under ideal sampling condition. Process simulation has allowed us to improve the performance of the survey method without expensive and lengthy field-work. In practice, the method would now be validated in the field probably by repeated sampling of communities in which prevalence is known from house-to-house screening.

## Cellular automata machines

*Cellular automata machines* are simple computing devices that are commonly used to simulate social, biological, and physical processes. Despite their simplicity, cellular automata machines are general purpose computing devices. This means that they may be used for any *computable* problem.

The way that problems are specified to cellular automata machines make them simple to program for some types of problem and difficult to program for other types of problem. In this exercise we will explore the use of cellular automata machines to create a simple model of epidemic spread.

Cellular automata machines model a universe in which space is represented by a uniform grid, time advances in steps, and the laws of the universe are represented by a set of rules which compute the future state of each cell of the grid from its current state and from the current state of its neighbouring cells.

Typically, a cellular automata machine has the following features:

1. It consists of a large number of identical cells arranged in a regular grid. The grid is a two-dimensional projection of a torus (a ring-doughnut shaped surface) and has no edges.

2. Each cell can be in one of a limited number of states.

3. Time advances through the simulation in steps. At each time-step, the state of a cell may change.

4. The state of a cell after each time-step is determined by a set of rules that define how the future state of a cell depends on the current state of the cell and the current state of its immediate neighbours. This set of rules is used to update the state of every cell in the grid at each time-step. Since the rules refer only to the state of an individual cell and its immediate neighbours, cellular automata machines are best suited to modelling situations where local interactions give rise to global phenomena.

In this exercise we will simulate a cellular automata machine using `R` and then use the simulated machine to simulate epidemic spread. We will use three functions to simulate the cellular automata machine (CAM):


- **cam.run():** This function will display the initial state of the CAM, examine each cell in the CAM grid, apply the CAM rule-set to each cell, update the CAM grid, and display the state of the CAM at each time-step.

- **cam.state.display():** This function will display the state of the CAM at each time-step. It will be implemented using the `image()` function to plot the contents of matrices held in the `cam.state` list object (see below). This function will be developed as we refine the epidemic model.

- **cam.rule():** This function will contain the rule-set. It will be implemented using `ifelse()` functions to codify rules. This function will also be developed as we refine the epidemic model.

The current and future states of the CAM will be held in two *global* list objects:

- **cam.state:** This object will contain the current state of the CAM and must contain a matrix object
called grid.

- **cam.state.new:** This object will contain the the state of the CAM at the next time-step as defined
by the the rule-set.

Other *global* objects will be defined as required.

Create a new function called `cam.run():` 

```{r, eval = FALSE}
cam.run <- function() {}
```

This creates an empty function called `cam.run()`.

Use the `fix()` function to edit the `cam.run()` function:

```{r, eval = FALSE}
fix(cam.run)
```

Edit the function to read:
   
```{r, eval = FALSE}
function(steps) {
  cam.state.new <<- cam.state
  cam.state.display(t = 0)
  mx <- nrow(cam.state$grid)
  my <- ncol(cam.state$grid)
  for(t in 1:steps) {
    for(y in 1:my) {
      for(x in 1:mx) {
        V <- cam.state$grid[x, y]
        N <- cam.state$grid[x, ifelse(y == 1, my, y - 1)]
        S <- cam.state$grid[x, ifelse(y == my, 1, y + 1)]
        E <- cam.state$grid[ifelse(x == mx, 1, x + 1), y]
        W <- cam.state$grid[ifelse(x == 1, mx, x - 1), y]
        cam.rule(V, N, S, E, W, x, y, t)
        }
      }
      cam.state <<- cam.state.new
      cam.state.display(t)
  }
}
```

```{r, echo = FALSE, eval = TRUE}
cam.run <- function(steps) {
  cam.state.new <<- cam.state
  cam.state.display(t = 0)
  mx <- nrow(cam.state$grid)
  my <- ncol(cam.state$grid)
  for(t in 1:steps) {
    for(y in 1:my) {
      for(x in 1:mx) {
        V <- cam.state$grid[x, y]
        N <- cam.state$grid[x, ifelse(y == 1, my, y - 1)]
        S <- cam.state$grid[x, ifelse(y == my, 1, y + 1)]
        E <- cam.state$grid[ifelse(x == mx, 1, x + 1), y]
        W <- cam.state$grid[ifelse(x == 1, mx, x - 1), y]
        cam.rule(V, N, S, E, W, x, y, t)
        }
      }
      cam.state <<- cam.state.new
      cam.state.display(t)
  }
}
```

Once you have made the changes shown above, check your work, save the file, and quit the 

Note that when we assign anything to the state of the CAM (held in `cam.state` and `cam.state.new`) we use the `<<-` (instead of the usual `<-`) assignment operator. This operator allows assignment to objects outside of the function in the global environment.

Objects that are stored in the *global* environment are available to all functions.

Create a new function called `cam.state.display()`: 

```{r, eval = FALSE}
cam.state.display <- function() {}
```

This creates an empty function called `cam.state.display()`.

Use the `fix()` function to edit the `cam.state.display()` function:

```{r, eval = FALSE}
fix(cam.state.display)
```

Edit the function to read:

```{r, eval = FALSE}   
function(t) {
  if(t == 0) {
    x11(); par(pty = "s")
  }
  image(cam.state$grid, main = paste("Infected at :", t),
        col = c("wheat", "navy"), axes = FALSE)
}
```

```{r, echo = FALSE, eval = TRUE}
cam.state.display <- function(t) {
  if(t == 0) {
    par(pty = "s")
  }
  image(cam.state$grid, main = paste("Infected at :", t),
        col = c("wheat", "navy"), axes = FALSE)
}
```

Remember that if you are using a Macintosh computer then you can use `quartz()` instead of `x11()`. This will give better results.

Once you have made the changes shown above, check your work, save the file, and quit the editor. 

The `cam.rule()` function contains the rules of the CAM universe.

We will start with a very simple *infection* rule:

* Each cell can be either *infected* or *not-infected*.

* If a cell is already *infected* it will remain *infected*.

* If a cell is *not-infected* then it will change its state to *infected* based on the state of its neighbours: If a neighbouring cell is *infected* it will infect the cell with a fixed probability or *transmission* pressure.

Create a new function called `cam.rule()`: 

```{r, eval = FALSE}
cam.rule <- function() {}
```

This creates an empty function called `cam.rule()`. Use the `fix()` function to edit the `cam.rule()` function:

```{r, eval = FALSE}
fix(cam.rule)
```

Edit the function to read:

```{r, eval = FALSE}
function(V, N, S, E, W, x, y, t) {
  tp <- c(N, S, E, W) * rbinom(4, 1, TP)
  cam.state.new$grid[x, y] <<- ifelse(V == 1 | sum(tp) > 0, 1, 0)
}
```

```{r, echo = FALSE, eval = TRUE}
cam.rule <- function(V, N, S, E, W, x, y, t) {
  tp <- c(N, S, E, W) * rbinom(4, 1, TP)
  cam.state.new$grid[x, y] <<- ifelse(V == 1 | sum(tp) > 0, 1, 0)
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor. The basic CAM machine is now complete.

We need to specify a value for the transmission pressure (`TP`): 

```{r, eval = TRUE}
TP <- 0.2
```

And define the initial state of the CAM:

```{r, eval = TRUE}
cases <- matrix(0, nrow = 19, ncol = 19)
cases[10, 10] <- 1
cam.state <- list(grid = cases)
```

We can now run the simulation:

```{r, eval = TRUE}
cam.run(steps = 20)
```

The number of infected cells is:

```{r, eval = TRUE}
sum(cam.state$grid)
```

We can use this model to investigate the effect of different transmission pressures by systematically altering the transmission pressure specified in `TP`:

```{r, eval = TRUE}
cases <- matrix(0, nrow = 19, ncol = 19)
cases[10, 10] <- 1
cam.state.initial <- list(grid = cases)
pressure <- vector(mode = "numeric")
infected <- vector(mode = "numeric")
   
for(TP in seq(0.05, 0.25, 0.05)) {
  cam.state <- cam.state.initial
  cam.run(steps = 20)
  graphics.off()
  pressure <- c(pressure, TP)
  infected <- c(infected, sum(cam.state$grid))
  }

plot(pressure, infected)
```

In practice we would run the simulation many times for each transmission pressure and plot (e.g.) the median number of infected cells found at the end of each run of the model.

We can extend the model to include host immunity by specifying a new layer of cells (i.e. for host immunity) and modifying the CAM rule-set appropriately.

Use the `fix()` function to edit the `cam.rule()` function:

```{r, eval = FALSE}
fix(cam.rule)
```

Edit the function to read:

```{r, eval = FALSE}
function(V, N, S, E, W, x, y, t) {
  tp <- c(N, S, E, W) * rbinom(4, 1, TP)
  cam.state.new$grid[x, y] <<- ifelse(V == 1 | (sum(tp) > 0 & cam.state$immune[x, y] != 1), 1, 0)
}
```

```{r, echo = FALSE, eval = TRUE}
cam.rule <- function(V, N, S, E, W, x, y, t) {
  tp <- c(N, S, E, W) * rbinom(4, 1, TP)
  cam.state.new$grid[x, y] <<- ifelse(V == 1 | (sum(tp) > 0 & cam.state$immune[x, y] != 1), 1, 0)
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor.

We need to specify values for the transmission pressure (`TP`) and the proportion of the population that is immune (`IM`):

```{r, eval = TRUE}
TP <- 0.2
IM <- 0.4
```

And define the initial state of the CAM:

```{r, eval = TRUE}
cases <- matrix(0, nrow = 19, ncol = 19)
cases[10, 10] <- 1
immune <- matrix(rbinom(361, 1, IM), nrow = 19, ncol = 19)
immune[10,10] <- 0
cam.state <- list(grid = cases, immune = immune)
```

We can display the distribution of immune cells using the `image()` function: 

```{r, eval = TRUE}
image(cam.state$immune, main = "Immune",
      col = c("wheat", "navy"), axes = FALSE)
```

We can now run the simulation:

```{r, eval = TRUE}
cam.run(steps = 30)
```

The number of infected cells is:

```{r, eval = TRUE}
sum(cam.state$grid)
```

A better summary is the proportion of susceptible (i.e. non-immune) cells that become infected during a run:

```{r, eval = TRUE}
sum(cam.state$grid) / (361 - sum(cam.state$immune))
```

We can use this model to investigate the effect of different proportions of the population that are immune by systematically altering the value assigned to `IM`:

```{r, eval = TRUE}
immune.p <- vector(mode = "numeric")
infected.p <- vector(mode = "numeric")

for(IM in seq(0, 0.8, 0.05)) {
  cases <-  matrix(0, nrow = 19, ncol = 19)
  cases[10, 10] <- 1
  immune <- matrix(rbinom(361, 1, IM), nrow = 19, ncol = 19)
  immune[10,10] <- 0
  cam.state <- list(grid = cases, immune = immune)
  cam.run(steps = 30)
  graphics.off()
  immune.p <- c(immune.p, sum(cam.state$immune) / 361)
  infected.p <- c(infected.p,
                  sum(cam.state$grid) / (361 - sum(cam.state$immune)))
}

plot(immune.p, infected.p)
```

In practice we would run the simulation many times for each value of `IM` and plot (e.g.) the median proportion of susceptible (i.e. non-immune) cells that become infected.

A simple modification to this model would be to record the time-step at which individual cells become infected.
We can do this by adding a new layer of cells (i.e. to record the time-step at which a cell becomes infected) and modifying the CAM rule-set appropriately.

Use the `fix()` function to edit the `cam.rule()` function: 

```{r, eval = FALSE}
fix(cam.rule)
```

Edit the function to read:

```{r, eval = FALSE}
function(V, N, S, E, W, x, y, t) {
  tp <- c(N, S, E, W) * rbinom(4, 1, TP)
  cam.state.new$grid[x, y] <<- ifelse(V == 1 | (sum(tp) > 0 & cam.state$immune[x, y] != 1), 1, 0)
  if(V != 1 & cam.state.new$grid[x, y] == 1) {
    cam.state.new$ti[x, y] <<- t
  }
}
```

```{r, echo = FALSE, eval = TRUE}
cam.rule <- function(V, N, S, E, W, x, y, t) {
  tp <- c(N, S, E, W) * rbinom(4, 1, TP)
  cam.state.new$grid[x, y] <<- ifelse(V == 1 | (sum(tp) > 0 & cam.state$immune[x, y] != 1), 1, 0)
  if(V != 1 & cam.state.new$grid[x, y] == 1) {
    cam.state.new$ti[x, y] <<- t
  }
}
```


Once you have made the changes shown above, check your work, save the file, and quit the editor.

We need to specify values for the transmission pressure (`TP`) and the proportion of the population that is immune (`IM`):

```{r, eval = TRUE}   
TP <- 0.2
IM <- 0.4
```

And define the initial state of the CAM:

```{r, eval = TRUE}
cases <- matrix(0, nrow = 19, ncol = 19)
cases[10, 10] <- 1
immune <- matrix(rbinom(361, 1, IM), nrow = 19, ncol = 19)
immune[10,10] <- 0
ti <- matrix(NA, nrow = 19, ncol = 19)
ti[10,10] <- 1
cam.state <- list(grid = cases, immune = immune, ti = ti)
```

We can now run the simulation:

```{r, eval = TRUE}
cam.run(steps = 120)
```

Recording the time-step at which individual cells become infected allows us to plot an epidemic curve from the model:

```{r, eval = FALSE}
x11()
hist(cam.state$ti)
```

```{r, echo = FALSE, eval = TRUE}
# x11()
hist(cam.state$ti)
```

Remember, if you are using a Macintosh computer then you can use `quartz()` instead of `x11()`. This will give better results.

The `image()` function can provide an alternative view of the same data: 

```{r, eval = TRUE}
image(cam.state$ti, axes = FALSE)
```

The colour of each cell reflects the time-step of infection (i.e. the darker cells were infected before the lighter cells).

The CAM models that we have developed are general models of an infectious phenomenon. They could, for example, be models of the spread of an item of gossip, a forest fire, or an ink-spot. They are, however, poorly specified models for the epidemic spread of an infectious disease. In particular, they assume that a cell is infectious to other cells immediately after infection, that an infected cell never loses its ability to infect other cells, recovery never takes place, and immunity is never acquired. These deficits in the models may be addressed by appropriate modification of the CAM rule-set.

A simple and useful model of epidemic spread is the *SIR* model. The letters in *SIR* refer to the three states that influence epidemic spread that an individual can exist in. The three states are **S**usceptible, **I**nfectious, and **R**ecovered.

We will now modify our CAM model to follow the *SIR* model using the following parameters:

* **Susceptible**: A cell may be immune or non-immune. A cell may be immune prior to the epidemic or
acquire immunity fourteen time-steps after infection. Once a cell is immune it remains immune. 

* **Infectious**: An infected cell is infectious from eight to fourteen time-steps after being infected.

* **Recovered**: A cell is clinically sick from ten to twenty time-steps after being infected. If one time-step is taken to equal one day, these parameters provide a coarse simulation of the course of a measles infection.

Use the `fix()` function to edit the `cam.rule()` function: 

```{r, eval = FALSE}
fix(cam.rule)
```

Edit the function to read:

```{r, eval = FALSE}
function(V, N, S, E, W, x, y, t) {
  tsi <- t - cam.state$ti[x, y]
  cam.state.new$grid[x, y] <<- ifelse(tsi %in% INFECTIOUS, 1, 0)
  cam.state.new$cf[x, y] <<- ifelse(tsi %in% CLINICAL, 1, 0)
  cam.state.new$immune[x, y] <<- ifelse(!is.na(tsi) &  tsi > IMMUNITY, 1, cam.state$immune[x,y])
     
  if(cam.state$infected[x, y] == 1) {
    cam.state.new$infected[x, y] <<- 1
    cam.state.new$ti[x, y] <<- cam.state$ti[x, y]
  } else {
    tp <- c(N, S, E, W) * rbinom(4, 1, TP)
    
    if(sum(tp) > 0 & cam.state$immune[x, y] != 1) {
      cam.state.new$infected[x, y] <<- 1
      cam.state.new$ti[x, y] <<- t
    }
  } 
}
```

```{r, echo = FALSE, eval = TRUE}
cam.rule <- function(V, N, S, E, W, x, y, t) {
  tsi <- t - cam.state$ti[x, y]
  cam.state.new$grid[x, y] <<- ifelse(tsi %in% INFECTIOUS, 1, 0)
  cam.state.new$cf[x, y] <<- ifelse(tsi %in% CLINICAL, 1, 0)
  cam.state.new$immune[x, y] <<- ifelse(!is.na(tsi) &  tsi > IMMUNITY, 1, cam.state$immune[x,y])
     
  if(cam.state$infected[x, y] == 1) {
    cam.state.new$infected[x, y] <<- 1
    cam.state.new$ti[x, y] <<- cam.state$ti[x, y]
  } else {
    tp <- c(N, S, E, W) * rbinom(4, 1, TP)
    
    if(sum(tp) > 0 & cam.state$immune[x, y] != 1) {
      cam.state.new$infected[x, y] <<- 1
      cam.state.new$ti[x, y] <<- t
    }
  } 
}
```

Once you have made the changes shown above, check your work, save the file, and quit the editor.

It will also be useful to have a more detailed report of the state of the CAM at each time-step. 

Use the `fix()` function to edit the `cam.state.display()` function:

```{r, eval = FALSE}
fix(cam.state.display)
```

Edit the function to read:

```{r, eval = FALSE}
function(t) {
  if(t == 0) {
    x11(width = 9, height = 9)
    par(mfrow = c(2, 2))
    par(pty = "s")
  }
     
  image(cam.state$grid,
        main = paste("Infectious at :", t),
        col = c("wheat", "navy"), axes = FALSE)
     
  image(cam.state$cf,
        main = paste("Clinical at :", t),
        col = c("wheat", "navy"), axes = FALSE)
     
  image(cam.state$ti,
        main = paste("Infected at :", t), axes = FALSE)
     
  image(cam.state$immune,
        main = paste("Immune at :", t),
        col = c("wheat", "navy"), axes = FALSE)
}
```

```{r, echo = FALSE, eval = TRUE}
cam.state.display <- function(t) {
  if(t == 0) {
    #x11(width = 9, height = 9)
    par(mfrow = c(2, 2))
    par(pty = "s")
  }
     
  image(cam.state$grid,
        main = paste("Infectious at :", t),
        col = c("wheat", "navy"), axes = FALSE)
     
  image(cam.state$cf,
        main = paste("Clinical at :", t),
        col = c("wheat", "navy"), axes = FALSE)
     
  image(cam.state$ti,
        main = paste("Infected at :", t), axes = FALSE)
     
  image(cam.state$immune,
        main = paste("Immune at :", t),
        col = c("wheat", "navy"), axes = FALSE)
}
```


Remember, if you are using a Macintosh computer then you can use `quartz()` instead of `x11()`. This will give better results.

Once you have made the changes shown above, check your work, save the file, and quit the editor.

We need to specify values for the transmission pressure (`TP`) and the proportion of the population that is
immune (`IM`):

```{r, eval = TRUE}
TP <- 0.2
IM <- 0.2
```

And the *SIR* parameters:

```{r, eval = TRUE}
CLINICAL <- 10:20
INFECTIOUS <- 8:14
IMMUNITY <- 14
```

And define the initial state of the CAM:

```{r, eval = TRUE}
infected <- matrix(0, nrow = 19, ncol = 19)
infected[10, 10] <- 1
ti <- matrix(NA, nrow = 19, ncol = 19)
ti[10,10] <- 0
infectious <- matrix(0, nrow = 19, ncol = 19)
immune <- matrix(rbinom(361, 1, IM), nrow = 19, ncol = 19)
immune[10,10] <- 0
cf <- matrix(0, nrow = 19, ncol = 19)
cam.state <-  list(grid = infectious, infected = infected, ti = ti,
                   immune = immune, cf = cf)
```

We should also record the number of susceptible cells for later use:

```{r, eval = TRUE}
susceptibles <- 361 - sum(cam.state$immune)
```

We can now run the simulation:

```{r, eval = TRUE}
cam.run(steps = 200)
```

We can now calculate the proportion of susceptible (i.e. non-immune) cells that become infected:

```{r, eval = TRUE}
sum(cam.state$infected) / susceptibles
```

We can use this model to test the effect of an intervention such as isolating an infected cell for a short period after clinical features first appear. We can do this, imperfectly because it does not allow us to specify compliance, by shortening the infectious period to include only the non-symptomatic time-steps and one time- step after clinical features have appeared:

```{r, eval = TRUE}
INFECTIOUS <- 8:11
```

resetting the initial state of the CAM:

```{r, eval  = TRUE}
infected <- matrix(0, nrow = 19, ncol = 19)
infected[10, 10] <- 1
ti <- matrix(NA, nrow = 19, ncol = 19)
ti[10,10] <- 0
infectious <- matrix(0, nrow = 19, ncol = 19)
immune <- matrix(rbinom(361, 1, IM), nrow = 19, ncol = 19)
immune[10,10] <- 0
cf <- matrix(0, nrow = 19, ncol = 19)
cam.state <- list(grid = infectious, infected = infected, ti = ti,
                  immune = immune, cf = cf)
```

Recording the number of susceptible cells:

```{r, eval = TRUE}
susceptibles <- 361 - sum(cam.state$immune)
```

Running the simulation:

```{r, eval = TRUE}
cam.run(steps = 200)
```

And recalculating the proportion of susceptible (i.e. non-immune) cells that become infected:

```{r, eval = TRUE}
sum(cam.state$infected) / susceptibles
```

We have developed a simple but realistic model of epidemic spread using a cellular automata machine. Such a model could be used to investigate the relative effect of model parameters (e.g. initial proportion immune, initial number of infective cells, transmission pressure, etc.) on epidemic spread by changing a single parameter at a time and running the simulation. Since the model is *stochastic*, the effect of each parameter change would be simulated many times and suitable summaries calculated.

The model could be improved by, for example:

* Allowing for an *open population* with births (or immigration) and deaths (or emigration). This could be implemented by allowing immune cells to become susceptible after a specified number of time-steps. The ratio of births to deaths could then be modelled as the ratio of the length of the infectious period to the length of the immune period.

* Specifying a non-uniform distribution of transmission pressure during the infectious period.

* Allowing for individual variation in susceptibility.

* Allowing for individual variation in the duration of the infectious period.

* Simulating a non-uniform population density by allowing cells to be empty. Note that an immune cell is the same as an empty cell in the current model.

* Simulating a clustered distribution of immunity in the initial state of the CAM.

* Allowing a small proportion of infections to be infectious without exhibiting clinical features.

* Specifying rules that are applied at different time-points such as introducing isolation only after a certain number of clinical cases have appeared (i.e. after an epidemic has been detected).

* Allowing the coverage of interventions (e.g. the coverage of a vaccination campaign or compliance with isolation instructions) that are introduced after an epidemic has been detected to be specified.

* Simulating a more complex social structure. This could be implemented by allowing cells to belong to one of a finite set of castes with different initial conditions (e.g. immunity) and having rules that specify the level of interaction (i.e. the transmission pressure) between members of separate castes and the levels of intervention coverage achievable in the separate castes.

* Extending the neighbourhood definition by using the corner (i.e. north-east, south-east, south-west, and north-west) cells as neighbours for consideration in the CAM rule-set.

We can now quit `R`: 

```{r, eval = FALSE}
q()
```

For this exercise there is no need to save the workspace image so click the **No** button (GUI) or enter `n` when prompted to save the workspace image (terminal).

## Summary

* Computer intensive methods provide an alternative to classical statistical techniques for both estimation and statistical hypothesis testing. They have the advantage of being simple to implement and they remain simple even with complex estimators.

* Computer based simulation can simulate both data and processes. Process simulations maybe arbitrarily complex. Process simulation is a useful development tool that can save considerable time and expense when developing systems and methods.

* Process simulation can also be used to model complex social phenomena such as epidemic spread. Such simulations allow (e.g.) the the relative efficacy of interventions to be evaluated.

* `R` provides functions that allow you to implement computer intensive methods such as the bootstrap and computer based simulation.